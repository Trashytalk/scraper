#!/usr/bin/env python3
"""
Test script to verify the CFPL intelligent crawling implementation
Updated to test the new Capture-First, Process-Later architecture
"""
import asyncio
import sys
import os

# Add the project directory to Python path
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from storage.cfpl_integration import CFPLScrapingEngine


async def test_cfpl_intelligent_crawl():
    """Test the CFPL intelligent crawl function"""
    
    print("ğŸ§ª Testing CFPL intelligent crawl functionality...")
    
    # Test configuration - smaller scale to verify it works
    test_config = {
        'max_pages': 5,  # Small number for quick test
        'max_depth': 2,
        'follow_internal_links': True,
        'follow_external_links': False,
        'immediate_processing': True  # For backward compatibility testing
    }
    
    # Test URL - use a simple site with internal links
    test_url = "https://en.wikipedia.org/wiki/Python_(programming_language)"
    
    print(f"ğŸ“„ Testing URL: {test_url}")
    print(f"âš™ï¸  Config: {test_config}")
    print("\nğŸš€ Starting CFPL intelligent crawl test...")
    
    try:
        # Initialize CFPL scraping engine
        async with CFPLScrapingEngine() as scraper:
            session_id = scraper.start_session("test_crawl_session")
            print(f"ğŸ”– Started session: {session_id}")
            
            # Call the intelligent_crawl function
            result = await scraper.intelligent_crawl(
                seed_url=test_url,
                scraper_type="intelligent",
                config=test_config
            )
            
            print(f"\nâœ… Crawl completed!")
            print(f"ğŸ“Š Results:")
            print(f"   - Status: {result.get('status', 'unknown')}")
            print(f"   - CFPL Enabled: {result.get('cfpl_enabled', False)}")
            print(f"   - Run ID: {result.get('run_id', 'N/A')}")
            print(f"   - Pages crawled: {len(result.get('crawled_data', []))}")
            print(f"   - Links discovered: {len(result.get('discovered_urls', []))}")
            print(f"   - Manifests created: {len(result.get('manifests', []))}")
            
            # Show CFPL-specific metrics
            summary = result.get('summary', {})
            print(f"   - Content stored: {summary.get('content_stored', 0)}")
            print(f"   - Processing errors: {summary.get('errors_encountered', 0)}")
            
            if result.get('crawled_data'):
                print(f"\nğŸ“„ Pages crawled (CFPL):")
                for i, page in enumerate(result['crawled_data'][:3]):  # Show first 3
                    print(f"   {i+1}. {page.get('url', 'unknown')}")
                    print(f"      Title: {page.get('title', 'N/A')}")
                    print(f"      Content hash: {page.get('content_hash', 'N/A')[:16]}...")
                    print(f"      Assets captured: {page.get('assets_captured', 0)}")
                    print(f"      Media captured: {page.get('media_captured', 0)}")
                    print(f"      Manifest: {page.get('manifest_path', 'N/A')}")
                    if page.get('word_count'):
                        print(f"      Word count: {page.get('word_count', 0)}")
            
            # Check if multiple pages were crawled
            pages_crawled = len(result.get('crawled_data', []))
            manifests_created = len(result.get('manifests', []))
            
            if pages_crawled > 1 and manifests_created > 0:
                print(f"\nğŸ‰ SUCCESS: CFPL crawled {pages_crawled} pages with {manifests_created} manifests!")
                print("   âœ“ Link following is working")
                print("   âœ“ Content is being stored in RAW zone")
                print("   âœ“ Processing pipeline is working")
                
                # Test storage stats
                stats = scraper.get_storage_stats()
                if stats:
                    print(f"\nğŸ“ˆ Storage Statistics:")
                    print(f"   - Total captures: {stats.get('captures', {}).get('total', 0)}")
                    print(f"   - Storage objects: {stats.get('storage', {}).get('total_objects', 0)}")
                    print(f"   - Deduplication savings: {stats.get('storage', {}).get('deduplication_savings', '0%')}")
                
                return True
            else:
                print(f"\nâš ï¸  WARNING: Only crawled {pages_crawled} page(s) - link following may not be working")
                return False
                
    except Exception as e:
        print(f"\nâŒ ERROR during CFPL crawl test: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


async def test_cfpl_single_url():
    """Test CFPL single URL capture"""
    print("\nğŸ¯ Testing CFPL single URL capture...")
    
    test_url = "https://httpbin.org/html"
    
    try:
        async with CFPLScrapingEngine() as scraper:
            result = await scraper.scrape_url(test_url, "basic")
            
            print(f"ğŸ“Š Single URL Results:")
            print(f"   - Status: {result.get('status', 'unknown')}")
            print(f"   - CFPL Enabled: {result.get('cfpl_enabled', False)}")
            print(f"   - Content Hash: {result.get('content_hash', 'N/A')[:16]}...")
            print(f"   - Assets Captured: {result.get('assets_captured', 0)}")
            print(f"   - Media Captured: {result.get('media_captured', 0)}")
            
            if result.get('status') == 'success' and result.get('cfpl_enabled'):
                print("âœ… Single URL CFPL capture works!")
                return True
            else:
                print("âŒ Single URL CFPL capture failed")
                return False
                
    except Exception as e:
        print(f"âŒ ERROR in single URL test: {str(e)}")
        return False


async def main():
    """Main test function"""
    print("ğŸ”§ Testing CFPL Implementation")
    print("=" * 50)
    
    # Test single URL first
    single_success = await test_cfpl_single_url()
    
    # Test intelligent crawl
    crawl_success = await test_cfpl_intelligent_crawl()
    
    print("\n" + "=" * 50)
    if single_success and crawl_success:
        print("âœ… ALL TESTS PASSED - CFPL implementation is working correctly!")
        print("\nğŸ¯ Key Features Verified:")
        print("   âœ“ Capture-First principle (content stored before processing)")
        print("   âœ“ Single-touch fetching (no refetching for analysis)")
        print("   âœ“ Content-addressed storage with deduplication")
        print("   âœ“ Immutable RAW zone with manifest tracking")
        print("   âœ“ Post-capture processing pipeline")
        print("   âœ“ Backward compatibility with existing interface")
    else:
        print("âŒ SOME TESTS FAILED - CFPL implementation needs investigation")
        if not single_success:
            print("   âŒ Single URL capture failed")
        if not crawl_success:
            print("   âŒ Intelligent crawl failed")
    
    return single_success and crawl_success


if __name__ == "__main__":
    success = asyncio.run(main())
    sys.exit(0 if success else 1)
