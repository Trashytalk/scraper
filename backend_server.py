#!/usr/bin/env python3
"""
Enhanced Backend API Server for Business Intelligence Scraper
Provides REST endpoints and WebSocket connections with advanced configuration management
"""

import asyncio
import hashlib
import json
import logging
import os
import sqlite3
import time
from datetime import datetime, timedelta
from typing import Any, Dict, List, Optional, Union

import jwt
import uvicorn

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)
from fastapi import (
    Depends,
    FastAPI,
    HTTPException,
    Request,
    WebSocket,
    WebSocketDisconnect,
)
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel

# Import centralized configuration
from config.environment import get_api_url, get_config as get_env_config, get_test_credentials

env_config = get_env_config()

# Import enhanced error handling
try:
    from error_handling.enhanced_error_handler import (
        ErrorCategory,
        ErrorSeverity,
        get_error_handler,
        handle_errors,
        init_error_handling,
    )

    ENHANCED_ERROR_HANDLING_AVAILABLE = True
except ImportError:
    logger.warning("Enhanced error handling not available")
    ENHANCED_ERROR_HANDLING_AVAILABLE = False
    
    # Fallback implementations
    def init_error_handling() -> Any:
        """Fallback error handling initialization"""
        logger.info("Using fallback error handling")
        return None

# Import enhanced monitoring
try:
    from monitoring.simple_health_monitor import (
        SimpleHealthMonitor,
        SimplePerformanceMiddleware,
        background_health_monitoring,
        get_health_monitor,
        init_simple_monitoring,
    )

    ENHANCED_MONITORING_AVAILABLE = True
except ImportError:
    logger.warning("Enhanced monitoring not available")
    ENHANCED_MONITORING_AVAILABLE = False
    
    # Fallback implementations
    def init_simple_monitoring(db_path: str = "data.db") -> Any:
        """Fallback monitoring initialization"""
        logger.info("Using fallback monitoring")
        return None
        
    async def background_health_monitoring() -> None:
        """Fallback background monitoring"""
        logger.info("Fallback background monitoring active")
        
    def get_health_monitor() -> Any:
        """Fallback health monitor"""
        return None

# Import AI integration (Phase 4)
try:
    from ml_pipeline import (
        AIIntegrationService,
        ai_service,
        create_ai_service,
        get_default_ai_service,
    )

    AI_INTEGRATION_AVAILABLE = True
    logger.info("ðŸ¤– AI Integration Service available")
except ImportError:
    logger.warning("AI Integration Service not available")
    AI_INTEGRATION_AVAILABLE = False

# Import enhanced configuration management
try:
    from config.advanced_config_manager import config_manager
    from config.advanced_config_manager import get_config as get_advanced_config
    from config.advanced_config_manager import init_config as init_advanced_config

    ADVANCED_CONFIG_AVAILABLE = True
    get_config = get_advanced_config  # Use advanced config if available
    init_config = init_advanced_config
except ImportError:
    logger.warning(
        "Advanced configuration manager not available, falling back to legacy config"
    )
    ADVANCED_CONFIG_AVAILABLE = False
    config_manager = None
    get_config = get_env_config  # Fallback to environment config

    async def init_config(*args, **kwargs) -> Any:
        return None


from slowapi import _rate_limit_exceeded_handler
from slowapi.errors import RateLimitExceeded

# Fallback classes for performance monitoring (defined early to avoid import issues)
class PerformanceMetrics:
    def __init__(self):
        self.request_metrics: Dict[str, List[Dict[str, Any]]] = {}
        self.system_metrics: Dict[str, Any] = {}
        self.start_time = time.time()

    def record_request(self, endpoint: str, duration: float, status_code: int) -> None:
        if endpoint not in self.request_metrics:
            self.request_metrics[endpoint] = []
        self.request_metrics[endpoint].append({
            "duration": duration,
            "status_code": status_code,
            "timestamp": time.time(),
        })
        if len(self.request_metrics[endpoint]) > 1000:
            self.request_metrics[endpoint] = self.request_metrics[endpoint][-1000:]

    def get_system_metrics(self) -> Dict[str, Any]:
        try:
            import psutil
            return {
                "cpu_percent": psutil.cpu_percent(),
                "memory_percent": psutil.virtual_memory().percent,
                "disk_usage": psutil.disk_usage('/').percent,
                "uptime": time.time() - self.start_time
            }
        except ImportError:
            return {"cpu_percent": 0, "memory_percent": 0, "disk_usage": 0, "uptime": time.time() - self.start_time}

    def get_endpoint_metrics(self) -> Dict[str, Dict[str, Any]]:
        metrics = {}
        for endpoint, requests in self.request_metrics.items():
            if requests:
                durations = [r["duration"] for r in requests]
                metrics[endpoint] = {
                    "total_requests": len(requests),
                    "avg_duration": sum(durations) / len(durations),
                    "min_duration": min(durations),
                    "max_duration": max(durations),
                }
        return metrics

    def get_recent_performance(self) -> List[Dict[str, Any]]:
        recent = []
        for endpoint, requests in self.request_metrics.items():
            recent.extend([{**r, "endpoint": endpoint} for r in requests[-10:]])
        return sorted(recent, key=lambda x: x["timestamp"], reverse=True)[:50]

class CacheManager:
    def __init__(self):
        self.cache: Dict[str, Any] = {}
        
    def get(self, key: str) -> Any:
        return self.cache.get(key)
        
    def set(self, key: str, value: Any, ttl: int = 300) -> None:
        self.cache[key] = value
        
    def delete(self, key: str) -> None:
        self.cache.pop(key, None)
        
    def clear(self) -> None:
        self.cache.clear()


# Import the real scraping engine
from scraping_engine import execute_scraping_job

# Import security components
from secure_config import (
    database_config,
    security_config,
    server_config,
    validate_configuration,
)
from security_middleware import (
    InputValidationMiddleware,
    RequestLoggingMiddleware,
    SecurityHeadersMiddleware,
    get_limiter,
    hash_password,
    validate_job_config,
    verify_password,
)

# Import performance monitoring (with fallback if dependencies missing)
try:
    from performance_monitor import (
        CacheManager as PerformanceMonitorCacheManager,
        DatabaseOptimizer as PerformanceMonitorDatabaseOptimizer,
        PerformanceMetrics as PerformanceMonitorMetrics,
        PerformanceMiddleware as PerformanceMonitorMiddleware,
        background_performance_monitor,
        cached,
        get_performance_summary,
        init_performance_system,
    )

    PERFORMANCE_ENABLED = True
    logger.info("âœ… Performance monitoring system available")
except ImportError:
    logger.warning(
        "âš ï¸  Performance monitoring dependencies not available - running with basic monitoring"
    )
    PERFORMANCE_ENABLED = False

# Global instances - will be initialized in startup
metrics_instance: Optional[Any] = None
cache_instance: Optional[Any] = None
db_optimizer_instance: Optional[Any] = None


def get_global_metrics() -> Optional[Any]:
    """Get the global metrics instance safely."""
    return metrics_instance


def get_global_cache() -> Optional[Any]:
    """Get the global cache instance safely."""
    return cache_instance


def get_global_db_optimizer() -> Optional[Any]:
    """Get the global database optimizer instance safely."""
    return db_optimizer_instance


# Fallback classes for when performance monitoring is not available
class SimpleFallbackMetrics:
        def __init__(self):
            self.ttl_cache: Dict[str, Any] = {}
            self.lru_cache: Dict[str, Any] = {}
            self.cache_times: Dict[str, float] = {}
            self.max_size = 1000

        def get(self, key: str, cache_type: str = "ttl") -> Any:
            if cache_type == "ttl":
                if key in self.ttl_cache and key in self.cache_times:
                    if time.time() - self.cache_times[key] < 300:  # 5 min default TTL
                        return self.ttl_cache[key]
                    else:
                        # Expired, remove
                        del self.ttl_cache[key]
                        del self.cache_times[key]
                        return None
                return self.ttl_cache.get(key)
            elif cache_type == "lru":
                if key in self.lru_cache:
                    # Move to end (most recently used)
                    value = self.lru_cache.pop(key)
                    self.lru_cache[key] = value
                    return value
            return None

        def set(self, key: str, value: Any, cache_type: str = "ttl", ttl: int = 300) -> None:
            if cache_type == "ttl":
                self.ttl_cache[key] = value
                self.cache_times[key] = time.time()

                # Simple cleanup of old entries
                if len(self.ttl_cache) > self.max_size:
                    # Remove oldest 10% of entries
                    oldest_keys = sorted(
                        self.cache_times.keys(), key=lambda k: self.cache_times[k]
                    )[: self.max_size // 10]
                    for old_key in oldest_keys:
                        self.ttl_cache.pop(old_key, None)
                        self.cache_times.pop(old_key, None)

            elif cache_type == "lru":
                if len(self.lru_cache) >= self.max_size:
                    # Remove least recently used
                    oldest_key = next(iter(self.lru_cache))
                    del self.lru_cache[oldest_key]
                self.lru_cache[key] = value

        def delete(self, key: str, cache_type: str = "ttl") -> None:
            if cache_type == "ttl":
                self.ttl_cache.pop(key, None)
                self.cache_times.pop(key, None)
            elif cache_type == "lru":
                self.lru_cache.pop(key, None)

        def clear_all(self) -> None:
            """Clear all caches"""
            self.ttl_cache.clear()
            self.lru_cache.clear()
            self.cache_times.clear()


class DatabaseOptimizer:
    """Fallback database optimizer"""
    def __init__(self):
        pass

    def optimize_query(self, query: str) -> str:
        return query

    def get_stats(self) -> Dict[str, Any]:
        return {"status": "fallback_optimizer", "optimizations": 0}


class PerformanceMiddleware:
        def __init__(self, app, metrics):
            self.app = app
            self.metrics = metrics

        async def __call__(self, request, call_next):
            start_time = time.time()
            response = await call_next(request)
            duration = time.time() - start_time

            # Record metrics
            endpoint = f"{request.method} {request.url.path}"
            self.metrics.record_request(endpoint, duration, response.status_code)

            return response


def cached(cache_type: str = "ttl", ttl: int = 300, key_prefix: str = ""):
        def decorator(func):
            async def wrapper(*args, **kwargs):
                # Simple caching decorator
                cache_key = (
                    f"{key_prefix}:{func.__name__}:{hash(str(args) + str(kwargs))}"
                )

                if hasattr(func, "_cache_manager"):
                    cached_result = func._cache_manager.get(cache_key, cache_type)
                    if cached_result is not None:
                        return cached_result

                result = (
                    await func(*args, **kwargs)
                    if asyncio.iscoroutinefunction(func)
                    else func(*args, **kwargs)
                )

                if hasattr(func, "_cache_manager"):
                    func._cache_manager.set(cache_key, result, cache_type, ttl)

                return result

            return wrapper

        return decorator


def init_performance_system(db_path: str, redis_url: Optional[str] = None) -> tuple[Any, Any, Any]:
    """Initialize performance system with fallback implementations"""
    metrics = PerformanceMetrics()
    cache_manager = CacheManager()
    db_optimizer = DatabaseOptimizer()
    
    # Store metrics globally for access
    global performance_metrics_instance
    performance_metrics_instance = metrics
    logger.info("âœ… Fallback performance system initialized")
    return metrics, cache_manager, db_optimizer


def get_performance_summary() -> Dict[str, Any]:
    if performance_metrics_instance:
        return {
            "system": performance_metrics_instance.get_system_metrics(),
            "endpoints": performance_metrics_instance.get_endpoint_metrics(),
            "recent": performance_metrics_instance.get_recent_performance(),
        }
    return {"status": "metrics not initialized"}


async def background_performance_monitor() -> None:
        """Basic background monitoring"""
        while True:
            try:
                await asyncio.sleep(60)  # Run every minute
                if performance_metrics_instance:
                    metrics = performance_metrics_instance.get_system_metrics()
                    if (
                        metrics.get("cpu_percent", 0) > 80
                        or metrics.get("memory_percent", 0) > 90
                    ):
                        logger.warning(
                            f"High resource usage detected: CPU={metrics.get('cpu_percent')}%, Memory={metrics.get('memory_percent')}%"
                        )
            except Exception as e:
                logger.error(f"Background monitoring error: {e}")

# Use the appropriate classes based on availability
# Global instances - will be initialized in startup
performance_metrics: Any = None
cache_manager: Any = None
db_optimizer: Any = None

# Helper function to safely access AI service
def get_ai_service():
    """Get AI service if available, otherwise raise exception"""
    if not AI_INTEGRATION_AVAILABLE:
        raise HTTPException(
            status_code=503, detail="AI integration not available"
        )
    # Check if ai_service is defined in the global scope
    try:
        return globals()['ai_service']
    except KeyError:
        raise HTTPException(
            status_code=503, detail="AI service not initialized"
        )


# Database setup
DATABASE_PATH = database_config.DATABASE_PATH


def init_database():
    """Initialize SQLite database with required tables"""
    os.makedirs(os.path.dirname(DATABASE_PATH), exist_ok=True)

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # Users table
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS users (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            username TEXT UNIQUE NOT NULL,
            email TEXT UNIQUE NOT NULL,
            password_hash TEXT NOT NULL,
            role TEXT DEFAULT 'viewer',
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            last_login TIMESTAMP,
            is_active BOOLEAN DEFAULT 1
        )
    """
    )

    # Jobs table
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS jobs (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            name TEXT NOT NULL,
            type TEXT NOT NULL,
            status TEXT DEFAULT 'pending',
            config TEXT,
            created_by INTEGER,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            started_at TIMESTAMP,
            completed_at TIMESTAMP,
            error_message TEXT,
            results_count INTEGER DEFAULT 0,
            FOREIGN KEY (created_by) REFERENCES users(id)
        )
    """
    )

    # Job results table
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS job_results (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            job_id INTEGER,
            data TEXT,
            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            FOREIGN KEY (job_id) REFERENCES jobs(id)
        )
    """
    )

    # Analytics table
    cursor.execute(
        """
        CREATE TABLE IF NOT EXISTS analytics (
            id INTEGER PRIMARY KEY AUTOINCREMENT,
            metric_name TEXT NOT NULL,
            metric_value REAL NOT NULL,
            timestamp TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
            metadata TEXT
        )
    """
    )

    # Create default admin user with secure password hashing
    current_config = get_config()
    default_password = getattr(current_config, 'DEFAULT_PASSWORD', 'admin123')
    default_username = getattr(current_config, 'DEFAULT_USERNAME', 'admin')
    
    password_hash = hash_password(default_password)
    cursor.execute(
        """
        INSERT OR IGNORE INTO users (username, email, password_hash, role)
        VALUES (?, ?, ?, ?)
    """,
        (default_username, "admin@scraper.local", password_hash, "admin"),
    )

    conn.commit()
    conn.close()


# Initialize database and validate configuration
validate_configuration()
init_database()

# Initialize performance system components with defaults
performance_metrics = None
cache_manager = None  
db_optimizer = None

# Initialize performance monitoring system
if PERFORMANCE_ENABLED:
    try:
        performance_metrics, cache_manager, db_optimizer = init_performance_system(
            DATABASE_PATH
        )
        logger.info("âœ… Performance monitoring system initialized")
    except Exception as e:
        logger.warning(f"âš ï¸  Performance monitoring initialization failed: {e}")
        PERFORMANCE_ENABLED = False
        performance_metrics = PerformanceMetrics()
        cache_manager = CacheManager()
        db_optimizer = DatabaseOptimizer()
else:
    performance_metrics = PerformanceMetrics()
    cache_manager = CacheManager()
    db_optimizer = DatabaseOptimizer()

# Rate limiter setup
limiter = get_limiter(security_config.API_RATE_LIMIT_PER_MINUTE)

# Global configuration storage
app_config = None

# FastAPI app setup
app = FastAPI(
    title="Business Intelligence Scraper API",
    description="Backend API for the BI Scraper Platform",
    version="2.0.0",
)


# Configuration startup and shutdown events
@app.on_event("startup")
async def startup_event():
    """Initialize application configuration and dependencies"""
    global app_config

    if ADVANCED_CONFIG_AVAILABLE:
        try:
            # Initialize advanced configuration management
            environment = os.getenv("ENVIRONMENT", "development")
            config_file = f"config/{environment}.yaml"

            if not os.path.exists(config_file):
                config_file = "config/development.yaml"  # Fallback

            app_config = await init_config(config_file, environment)
            if config_manager and app_config:
                await config_manager.start_watching()

                logging.info(
                    f"âœ… Advanced configuration loaded (environment: {environment})"
                )
                logging.info(f"   Database: {app_config.database.url}")
                logging.info(
                    f"   Redis: {app_config.redis.host}:{app_config.redis.port}"
                )
                logging.info(f"   Log Level: {app_config.monitoring.log_level}")

                # Configure logging based on config
                logging.getLogger().setLevel(
                    getattr(logging, app_config.monitoring.log_level)
                )

        except Exception as e:
            logging.error(f"âŒ Failed to load advanced configuration: {e}")
            logging.info("Falling back to legacy configuration")
            app_config = None

    # Initialize database
    init_database()

    # Initialize enhanced monitoring if available
    if ENHANCED_MONITORING_AVAILABLE:
        try:
            if 'init_simple_monitoring' in globals():
                monitor = init_simple_monitoring(DATABASE_PATH)
            if 'background_health_monitoring' in globals():
                asyncio.create_task(background_health_monitoring())
            logger.info("âœ… Enhanced health monitoring initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize enhanced monitoring: {e}")

    # Initialize enhanced error handling if available
    if ENHANCED_ERROR_HANDLING_AVAILABLE:
        try:
            if 'init_error_handling' in globals():
                error_handler = init_error_handling()
            logger.info("âœ… Enhanced error handling initialized")
        except Exception as e:
            logger.error(f"âŒ Failed to initialize enhanced error handling: {e}")

    # Initialize performance monitoring background task
    if PERFORMANCE_ENABLED and 'background_performance_monitor' in globals():
        try:
            asyncio.create_task(background_performance_monitor())
            logger.info("âœ… Performance monitoring background task started")
        except Exception as e:
            logger.error(f"âŒ Failed to start performance monitoring: {e}")


@app.on_event("shutdown")
async def shutdown_event():
    """Cleanup resources on shutdown"""
    if ADVANCED_CONFIG_AVAILABLE and config_manager:
        try:
            await config_manager.stop_watching()
            logging.info("âœ… Configuration manager stopped")
        except Exception as e:
            logging.error(f"Error stopping config manager: {e}")


# Helper function to get current configuration
def get_current_config():
    """Get current application configuration"""
    if app_config:
        return app_config
    # Fallback to legacy config
    return None


# Add security middleware
app.add_middleware(
    SecurityHeadersMiddleware,
    enable_headers=security_config.ENABLE_SECURITY_HEADERS,
    hsts_max_age=security_config.HSTS_MAX_AGE,
)
app.add_middleware(InputValidationMiddleware)
app.add_middleware(RequestLoggingMiddleware)

# Add performance monitoring middleware
if PERFORMANCE_ENABLED:
    from starlette.middleware.base import BaseHTTPMiddleware

    class PerformanceMiddlewareWrapper(BaseHTTPMiddleware):
        def __init__(self, app, metrics):
            super().__init__(app)
            self.metrics = metrics

        async def dispatch(self, request, call_next):
            start_time = time.time()
            response = await call_next(request)
            duration = time.time() - start_time

            # Record metrics if available
            if hasattr(self.metrics, "record_request"):
                endpoint = f"{request.method} {request.url.path}"
                self.metrics.record_request(endpoint, duration, response.status_code)

            response.headers["X-Response-Time"] = f"{duration:.3f}s"
            return response

    app.add_middleware(PerformanceMiddlewareWrapper, metrics=performance_metrics)

# CORS middleware with secure configuration
app.add_middleware(
    CORSMiddleware,
    allow_origins=security_config.CORS_ORIGINS,
    allow_credentials=True,
    allow_methods=["GET", "POST", "PUT", "DELETE"],
    allow_headers=["*"],
)

# Rate limiting
app.state.limiter = limiter
# Note: Rate limit exception handler disabled due to type conflicts


# Startup event for background monitoring
@app.on_event("startup")
async def startup_background_monitoring():
    """Initialize background tasks on startup"""
    if PERFORMANCE_ENABLED:
        # Start background performance monitoring
        asyncio.create_task(background_performance_monitor())
        print("ðŸ”„ Background performance monitoring started")


# JWT Configuration from secure config
JWT_SECRET = security_config.JWT_SECRET
JWT_ALGORITHM = security_config.JWT_ALGORITHM


# Custom security dependency to return 401 instead of 403
async def get_bearer_token(request: Request):
    authorization = request.headers.get("Authorization")
    if not authorization:
        raise HTTPException(status_code=401, detail="Authorization header missing")

    try:
        scheme, token = authorization.split()
        if scheme.lower() != "bearer":
            raise HTTPException(status_code=401, detail="Invalid authentication scheme")
        return token
    except ValueError:
        raise HTTPException(
            status_code=401, detail="Invalid authorization header format"
        )


# WebSocket connection manager
class ConnectionManager:
    def __init__(self):
        self.active_connections: List[WebSocket] = []

    async def connect(self, websocket: WebSocket):
        await websocket.accept()
        self.active_connections.append(websocket)

    def disconnect(self, websocket: WebSocket):
        self.active_connections.remove(websocket)

    async def send_personal_message(self, message: str, websocket: WebSocket):
        await websocket.send_text(message)

    async def broadcast(self, message: str):
        for connection in self.active_connections:
            try:
                await connection.send_text(message)
            except:
                pass


manager = ConnectionManager()


# Pydantic models
class UserLogin(BaseModel):
    username: str
    password: str


class UserResponse(BaseModel):
    id: int
    username: str
    email: str
    role: str
    created_at: str


class JobCreate(BaseModel):
    name: str
    type: str  # Basic job type for backward compatibility
    config: Dict[str, Any] = {}
    url: Optional[str] = None  # URL to scrape
    scraper_type: Optional[str] = (
        "basic"  # Scraper type: basic, e_commerce, news, social_media, api
    )
    custom_selectors: Optional[Dict[str, str]] = (
        None  # Custom CSS selectors for data extraction
    )


class JobResponse(BaseModel):
    id: int
    name: str
    type: str
    status: str
    created_at: str
    results_count: int


class BatchJobCreate(BaseModel):
    base_name: str
    urls: List[str]
    scraper_type: Optional[str] = "basic"
    batch_size: Optional[int] = 10
    config: Optional[Dict[str, Any]] = {}


class AnalyticsData(BaseModel):
    metric_name: str
    metric_value: float
    metadata: Optional[Dict[str, Any]] = None


# Authentication functions
def verify_user_password(plain_password: str, hashed_password: str) -> bool:
    """Verify a password against its hash"""
    return verify_password(plain_password, hashed_password)


def create_access_token(data: dict, expires_delta: Optional[timedelta] = None):
    """Create a JWT access token"""
    to_encode = data.copy()
    if expires_delta:
        expire = datetime.utcnow() + expires_delta
    else:
        expire = datetime.utcnow() + timedelta(hours=security_config.JWT_EXPIRE_HOURS)
    to_encode.update({"exp": expire})
    encoded_jwt = jwt.encode(to_encode, JWT_SECRET, algorithm=JWT_ALGORITHM)
    return encoded_jwt


def get_current_user(token: str = Depends(get_bearer_token)):
    """Get current authenticated user"""
    try:
        payload = jwt.decode(token, JWT_SECRET, algorithms=[JWT_ALGORITHM])
        username: str = payload.get("sub")
        if username is None:
            raise HTTPException(
                status_code=401, detail="Invalid authentication credentials"
            )

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute(
            "SELECT * FROM users WHERE username = ? AND is_active = 1", (username,)
        )
        user = cursor.fetchone()
        conn.close()

        if user is None:
            raise HTTPException(status_code=401, detail="User not found")

        return {"id": user[0], "username": user[1], "email": user[2], "role": user[4]}
    except jwt.PyJWTError:
        raise HTTPException(
            status_code=401, detail="Invalid authentication credentials"
        )


# API Endpoints


@app.get("/api/health")
async def health_check():
    """Enhanced health check endpoint with comprehensive monitoring"""
    health_data = {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "version": "2.0.0",
        "environment": os.getenv("ENVIRONMENT", "development"),
    }

    # Enhanced monitoring if available
    if ENHANCED_MONITORING_AVAILABLE:
        try:
            monitor = get_health_monitor()
            if monitor:
                comprehensive_health = await monitor.comprehensive_health_check()
                health_data.update(comprehensive_health)
                logging.debug("Enhanced health check completed")
        except Exception as e:
            logging.error(f"Enhanced health check failed: {e}")
            health_data["enhanced_monitoring_error"] = str(e)

    # Legacy performance monitoring fallback
    elif PERFORMANCE_ENABLED:
        try:
            health_data["performance"] = performance_metrics.get_recent_performance()
            health_data["system"] = performance_metrics.get_system_metrics()
        except Exception as e:
            logging.error(f"Legacy performance check failed: {e}")
            health_data["performance_error"] = str(e)

    return health_data


@app.post("/api/auth/login")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def login(request: Request, user_data: UserLogin):
    """User authentication endpoint with rate limiting"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute(
        "SELECT * FROM users WHERE username = ? AND is_active = 1",
        (user_data.username,),
    )
    user = cursor.fetchone()

    if not user or not verify_user_password(user_data.password, user[3]):
        conn.close()
        raise HTTPException(status_code=401, detail="Invalid username or password")

    # Update last login
    cursor.execute(
        "UPDATE users SET last_login = CURRENT_TIMESTAMP WHERE id = ?", (user[0],)
    )
    conn.commit()
    conn.close()

    # Create access token
    access_token = create_access_token(data={"sub": user[1]})

    return {
        "access_token": access_token,
        "token_type": "bearer",
        "user": {"id": user[0], "username": user[1], "email": user[2], "role": user[4]},
    }


@app.get("/api/auth/me")
async def get_current_user_info(current_user: dict = Depends(get_current_user)):
    """Get current user information"""
    return current_user


@app.post("/api/jobs")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def create_job(
    request: Request,
    job_data: JobCreate,
    current_user: dict = Depends(get_current_user),
):
    """Create a new scraping job with input validation"""

    # Build the complete job configuration
    job_config = {
        "url": job_data.url,
        "scraper_type": job_data.scraper_type or "basic",
        "config": {
            "custom_selectors": job_data.custom_selectors or {},
            **job_data.config,  # Merge any additional config
        },
    }

    # Validate job configuration for security
    try:
        validated_config = validate_job_config(job_config)
    except ValueError as e:
        raise HTTPException(status_code=400, detail=str(e))

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute(
        """
        INSERT INTO jobs (name, type, config, created_by, status)
        VALUES (?, ?, ?, ?, 'pending')
    """,
        (
            job_data.name,
            job_data.type,
            json.dumps(validated_config),
            current_user["id"],
        ),
    )
    job_id = cursor.lastrowid
    conn.commit()
    conn.close()

    # Broadcast job creation to WebSocket clients
    await manager.broadcast(
        json.dumps(
            {
                "type": "job_created",
                "data": {
                    "id": job_id,
                    "name": job_data.name,
                    "type": job_data.type,
                    "url": job_data.url,
                    "scraper_type": job_data.scraper_type,
                    "status": "pending",
                },
            }
        )
    )

    return {"id": job_id, "message": "Job created successfully"}


@app.post("/api/jobs/batch")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def create_batch_jobs(
    request: Request,
    batch_data: BatchJobCreate,
    current_user: dict = Depends(get_current_user),
):
    """Create multiple scraping jobs from a list of URLs"""

    if not batch_data.urls:
        raise HTTPException(status_code=400, detail="No URLs provided")

    if len(batch_data.urls) > 100:  # Safety limit
        raise HTTPException(status_code=400, detail="Too many URLs (max 100)")

    created_jobs = []
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    try:
        for i, url in enumerate(batch_data.urls):
            # Create job name with index
            job_name = f"{batch_data.base_name} - Job {i + 1}"

            # Build job configuration
            job_config = {
                "url": url,
                "scraper_type": batch_data.scraper_type or "basic",
                "config": batch_data.config or {},
            }

            # Validate job configuration for security
            try:
                validated_config = validate_job_config(job_config)
            except ValueError as e:
                raise HTTPException(
                    status_code=400, detail=f"Invalid config for URL {url}: {str(e)}"
                )

            # Insert job into database
            cursor.execute(
                """
                INSERT INTO jobs (name, type, config, created_by, status)
                VALUES (?, ?, ?, ?, 'pending')
                """,
                (
                    job_name,
                    "batch_scraping",
                    json.dumps(validated_config),
                    current_user["id"],
                ),
            )
            job_id = cursor.lastrowid

            created_jobs.append(
                {
                    "id": job_id,
                    "name": job_name,
                    "url": url,
                    "scraper_type": batch_data.scraper_type,
                    "status": "pending",
                }
            )

        conn.commit()

        # Broadcast batch job creation to WebSocket clients
        await manager.broadcast(
            json.dumps(
                {
                    "type": "batch_jobs_created",
                    "data": {
                        "batch_name": batch_data.base_name,
                        "jobs_count": len(created_jobs),
                        "jobs": created_jobs,
                    },
                }
            )
        )

        return {
            "message": f"Successfully created {len(created_jobs)} batch jobs",
            "batch_name": batch_data.base_name,
            "jobs_created": len(created_jobs),
            "jobs": created_jobs,
        }

    except Exception as e:
        conn.rollback()
        raise HTTPException(
            status_code=500, detail=f"Failed to create batch jobs: {str(e)}"
        )
    finally:
        conn.close()


@app.get("/api/jobs/{job_id}")
async def get_job(job_id: int, current_user: dict = Depends(get_current_user)):
    """Get specific job details"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute(
        """
        SELECT * FROM jobs 
        WHERE id = ? AND created_by = ?
    """,
        (job_id, current_user["id"]),
    )
    job = cursor.fetchone()
    conn.close()

    if not job:
        raise HTTPException(status_code=404, detail="Job not found")

    return {
        "id": job[0],
        "name": job[1],
        "type": job[2],
        "status": job[3],
        "config": json.loads(job[4]) if job[4] else {},
        "created_at": job[6],
        "results_count": job[9] or 0,
    }


@app.post("/api/jobs/{job_id}/start")
async def start_job(job_id: int, current_user: dict = Depends(get_current_user)):
    """Start a scraping job"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # First, get the job configuration and type
    cursor.execute(
        """
        SELECT config, type FROM jobs 
        WHERE id = ? AND created_by = ?
    """,
        (job_id, current_user["id"]),
    )

    job_row = cursor.fetchone()
    if not job_row:
        conn.close()
        raise HTTPException(status_code=404, detail="Job not found")

    job_config = json.loads(job_row[0]) if job_row[0] else {}
    job_type = job_row[1]

    # Include the job type in the config so the scraping engine knows what to do
    job_config["type"] = job_type

    # Update job status to running
    cursor.execute(
        """
        UPDATE jobs 
        SET status = 'running', started_at = CURRENT_TIMESTAMP 
        WHERE id = ?
    """,
        (job_id,),
    )
    conn.commit()
    conn.close()

    # Execute real scraping job in background
    asyncio.create_task(execute_real_scraping_job(job_id, job_config))

    return {"message": "Job started successfully"}


async def execute_real_scraping_job(job_id: int, job_config: Dict[str, Any]):
    """Execute real scraping job using the scraping engine"""
    try:
        # Update job status to running
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()
        cursor.execute(
            """
            UPDATE jobs 
            SET status = 'running'
            WHERE id = ?
        """,
            (job_id,),
        )
        conn.commit()
        conn.close()

        # Broadcast job start
        await manager.broadcast(
            json.dumps(
                {"type": "job_started", "data": {"id": job_id, "status": "running"}}
            )
        )

        # Execute the scraping job
        result = await execute_scraping_job(job_id, job_config)

        # Broadcast job completion
        await manager.broadcast(
            json.dumps(
                {
                    "type": "job_completed",
                    "data": {
                        "id": job_id,
                        "status": result["status"],
                        "url": result.get("url", ""),
                        "timestamp": result.get("timestamp", ""),
                    },
                }
            )
        )

    except Exception as e:
        # Handle any errors during job execution
        print(f"Error executing job {job_id}: {str(e)}")

        # Update job status to failed
        try:
            conn = sqlite3.connect(DATABASE_PATH)
            cursor = conn.cursor()
            cursor.execute(
                """
                UPDATE jobs 
                SET status = 'failed', 
                    completed_at = CURRENT_TIMESTAMP,
                    error_message = ?
                WHERE id = ?
            """,
                (str(e), job_id),
            )
            conn.commit()
            conn.close()

            # Broadcast job failure
            await manager.broadcast(
                json.dumps(
                    {
                        "type": "job_failed",
                        "data": {"id": job_id, "status": "failed", "error": str(e)},
                    }
                )
            )
        except Exception as db_error:
            print(f"Failed to update job status: {str(db_error)}")


@app.get("/api/jobs/{job_id}/results")
async def get_job_results(job_id: int, current_user: dict = Depends(get_current_user)):
    """Get results for a specific job"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # First check if job exists and belongs to user
    cursor.execute(
        """
        SELECT id FROM jobs 
        WHERE id = ? AND created_by = ?
    """,
        (job_id, current_user["id"]),
    )

    if not cursor.fetchone():
        conn.close()
        raise HTTPException(status_code=404, detail="Job not found")

    # Get job results
    cursor.execute(
        """
        SELECT data, created_at FROM job_results 
        WHERE job_id = ?
        ORDER BY created_at DESC
    """,
        (job_id,),
    )

    results = []
    for row in cursor.fetchall():
        try:
            data = json.loads(row[0])
            data["retrieved_at"] = row[1]
            results.append(data)
        except json.JSONDecodeError:
            # Handle any JSON parsing errors
            results.append(
                {
                    "error": "Failed to parse result data",
                    "raw_data": row[0],
                    "retrieved_at": row[1],
                }
            )

    conn.close()
    return results


async def simulate_job_execution(job_id: int):
    """Legacy simulation method - kept for backward compatibility"""
    await asyncio.sleep(2)  # Simulate processing time

    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # Update job status to completed
    cursor.execute(
        """
        UPDATE jobs 
        SET status = 'completed', completed_at = CURRENT_TIMESTAMP, results_count = ?
        WHERE id = ?
    """,
        (42, job_id),
    )  # Simulate 42 results

    # Add some sample results
    sample_data = [
        {
            "url": "https://example.com/page1",
            "title": "Sample Page 1",
            "content": "Sample content 1",
        },
        {
            "url": "https://example.com/page2",
            "title": "Sample Page 2",
            "content": "Sample content 2",
        },
    ]

    for data in sample_data:
        cursor.execute(
            """
            INSERT INTO job_results (job_id, data)
            VALUES (?, ?)
        """,
            (job_id, json.dumps(data)),
        )

    conn.commit()
    conn.close()

    # Broadcast job completion
    await manager.broadcast(
        json.dumps(
            {
                "type": "job_completed",
                "data": {"id": job_id, "status": "completed", "results_count": 42},
            }
        )
    )


@app.get("/api/analytics/dashboard")
async def get_dashboard_analytics(current_user: dict = Depends(get_current_user)):
    """Get analytics data for dashboard"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()

    # Get job statistics
    cursor.execute(
        """
        SELECT status, COUNT(*) 
        FROM jobs 
        WHERE created_by = ? 
        GROUP BY status
    """,
        (current_user["id"],),
    )
    job_stats = dict(cursor.fetchall())

    # Get total results
    cursor.execute(
        """
        SELECT SUM(results_count) 
        FROM jobs 
        WHERE created_by = ? AND status = 'completed'
    """,
        (current_user["id"],),
    )
    total_results = cursor.fetchone()[0] or 0

    conn.close()

    return {
        "jobs": {
            "total": sum(job_stats.values()),
            "running": job_stats.get("running", 0),
            "completed": job_stats.get("completed", 0),
            "failed": job_stats.get("failed", 0),
            "pending": job_stats.get("pending", 0),
        },
        "results": {
            "total": total_results,
            "today": 15,  # Simulated
            "this_week": 127,  # Simulated
        },
        "performance": {
            "avg_processing_time": "2.3s",
            "success_rate": "94.2%",
            "data_quality_score": "87.5%",
        },
    }


@app.get("/api/analytics/metrics")
async def get_analytics_metrics(current_user: dict = Depends(get_current_user)):
    """Get detailed analytics metrics for charts"""
    # Simulate time-series data for charts
    import random
    from datetime import datetime, timedelta

    # Generate sample data for the last 30 days
    end_date = datetime.now()
    dates = [
        (end_date - timedelta(days=i)).strftime("%Y-%m-%d") for i in range(30, 0, -1)
    ]

    metrics = {
        "job_completion_trend": [
            {
                "date": date,
                "completed": random.randint(5, 25),
                "failed": random.randint(0, 3),
            }
            for date in dates
        ],
        "data_volume_trend": [
            {"date": date, "volume": random.randint(100, 1000)} for date in dates
        ],
        "performance_metrics": [
            {
                "date": date,
                "response_time": random.uniform(1.0, 5.0),
                "success_rate": random.uniform(85, 99),
            }
            for date in dates
        ],
        "top_sources": [
            {"source": "example.com", "count": 156},
            {"source": "sample.org", "count": 89},
            {"source": "demo.net", "count": 67},
            {"source": "test.io", "count": 45},
        ],
    }

    return metrics


@app.get("/api/performance/summary")
async def get_performance_summary_endpoint(
    current_user: dict = Depends(get_current_user),
):
    """Get comprehensive performance summary"""
    if not PERFORMANCE_ENABLED:
        return {"error": "Performance monitoring not available", "enabled": False}

    return get_performance_summary()


@app.get("/api/performance/metrics")
async def get_performance_metrics(current_user: dict = Depends(get_current_user)):
    """Get real-time performance metrics"""
    if not PERFORMANCE_ENABLED:
        return {"error": "Performance monitoring not available", "enabled": False}

    return {
        "system": performance_metrics.get_system_metrics(),
        "endpoints": performance_metrics.get_endpoint_metrics(),
        "recent_5min": performance_metrics.get_recent_performance(5),
        "recent_1min": performance_metrics.get_recent_performance(1),
        "timestamp": datetime.utcnow().isoformat(),
    }


@app.get("/api/performance/cache/stats")
async def get_cache_stats(current_user: dict = Depends(get_current_user)):
    """Get cache performance statistics"""
    if not PERFORMANCE_ENABLED:
        return {"error": "Performance monitoring not available", "enabled": False}

    # Cache stats would be implemented based on actual cache manager
    return {
        "cache_enabled": True,
        "redis_available": getattr(cache_manager, "redis_available", False),
        "ttl_cache_size": len(getattr(cache_manager, "ttl_cache", {})),
        "lru_cache_size": len(getattr(cache_manager, "lru_cache", {})),
        "job_cache_size": len(getattr(cache_manager, "job_cache", {})),
        "timestamp": datetime.utcnow().isoformat(),
    }


@app.post("/api/performance/cache/clear")
async def clear_cache(current_user: dict = Depends(get_current_user)):
    """Clear all caches (admin only)"""
    if current_user.get("role") != "admin":
        raise HTTPException(status_code=403, detail="Admin access required")

    if not PERFORMANCE_ENABLED:
        return {"error": "Performance monitoring not available", "enabled": False}

    try:
        cache_manager.clear_all()
        return {"message": "All caches cleared successfully"}
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"Error clearing cache: {str(e)}")


@app.get("/api/jobs", response_model=List[JobResponse])
@cached(cache_type="ttl", ttl=60, key_prefix="jobs_")
async def get_jobs(current_user: dict = Depends(get_current_user)):
    """Get all jobs for the current user with caching"""
    conn = sqlite3.connect(DATABASE_PATH)
    cursor = conn.cursor()
    cursor.execute(
        """
        SELECT id, name, type, status, created_at, results_count 
        FROM jobs 
        WHERE created_by = ? 
        ORDER BY created_at DESC
    """,
        (current_user["id"],),
    )
    jobs = cursor.fetchall()
    conn.close()

    return [
        JobResponse(
            id=job[0],
            name=job[1],
            type=job[2],
            status=job[3],
            created_at=job[4],
            results_count=job[5] or 0,
        )
        for job in jobs
    ]


@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint for real-time updates"""
    await manager.connect(websocket)
    try:
        while True:
            # Send periodic updates
            await asyncio.sleep(5)
            await websocket.send_text(
                json.dumps(
                    {
                        "type": "metrics_update",
                        "data": {
                            "timestamp": datetime.utcnow().isoformat(),
                            "active_jobs": 3,
                            "queue_size": 7,
                            "processing_rate": "12.5/min",
                        },
                    }
                )
            )
    except WebSocketDisconnect:
        manager.disconnect(websocket)


# Data centralization endpoint
class CentralizeDataRequest(BaseModel):
    job_id: int
    job_name: str
    data: List[Dict[str, Any]]
    metadata: Dict[str, Any]


@app.post("/api/data/centralize")
async def centralize_data(
    request: CentralizeDataRequest, current_user: dict = Depends(get_current_user)
):
    """
    Centralize scraped data for analytics and storage
    Enhanced with comprehensive data processing and quality metrics
    """
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        # Create centralized data table if it doesn't exist
        cursor.execute(
            """
            CREATE TABLE IF NOT EXISTS centralized_data (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                source_job_id INTEGER,
                source_job_name TEXT,
                source_job_type TEXT,
                source_url TEXT,
                raw_data TEXT,
                processed_data TEXT,
                data_type TEXT,
                content_hash TEXT,
                scraped_at TIMESTAMP,
                centralized_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
                data_quality_score INTEGER,
                completeness_score INTEGER,
                validation_status TEXT,
                word_count INTEGER,
                link_count INTEGER,
                image_count INTEGER,
                crawl_metadata TEXT
            )
        """
        )

        # Create index for efficient lookups
        cursor.execute(
            """
            CREATE INDEX IF NOT EXISTS idx_content_hash 
            ON centralized_data(content_hash)
        """
        )

        centralized_count = 0
        duplicate_count = 0

        for item in request.data:
            # Calculate content hash for deduplication
            content_str = json.dumps(item, sort_keys=True)
            content_hash = hashlib.md5(content_str.encode()).hexdigest()

            # Check for duplicates
            cursor.execute(
                "SELECT id FROM centralized_data WHERE content_hash = ?",
                (content_hash,),
            )
            existing = cursor.fetchone()

            if existing:
                duplicate_count += 1
                continue

            # Calculate quality metrics
            quality_score = 0
            completeness_score = 0
            word_count = 0
            link_count = 0
            image_count = 0

            # Quality assessment
            if item.get("title"):
                quality_score += 20
                completeness_score += 25
            if item.get("content") or item.get("text_content"):
                content = item.get("content", "") or item.get("text_content", "")
                if len(content) > 100:
                    quality_score += 30
                    completeness_score += 25
                word_count = len(content.split())
            if item.get("url"):
                quality_score += 20
                completeness_score += 20
            if item.get("links"):
                links = item.get("links", [])
                link_count = len(links) if isinstance(links, list) else 0
                if link_count > 0:
                    quality_score += 15
                    completeness_score += 15
            if item.get("images"):
                images = item.get("images", [])
                image_count = len(images) if isinstance(images, list) else 0
                if image_count > 0:
                    quality_score += 15
                    completeness_score += 15

            # Determine data type
            data_type = "general"
            content_lower = str(item).lower()
            if any(
                keyword in content_lower
                for keyword in ["product", "price", "buy", "cart"]
            ):
                data_type = "ecommerce"
            elif any(
                keyword in content_lower
                for keyword in ["article", "news", "headline", "author"]
            ):
                data_type = "news"
            elif any(
                keyword in content_lower
                for keyword in ["post", "tweet", "comment", "like"]
            ):
                data_type = "social_media"

            # Process crawl metadata if available
            crawl_metadata = ""
            if item.get("crawl_metadata"):
                crawl_metadata = json.dumps(item["crawl_metadata"])

            # Insert centralized record
            cursor.execute(
                """
                INSERT INTO centralized_data (
                    source_job_id, source_job_name, source_job_type, source_url,
                    raw_data, processed_data, data_type, content_hash,
                    scraped_at, data_quality_score, completeness_score,
                    validation_status, word_count, link_count, image_count,
                    crawl_metadata
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            """,
                (
                    request.job_id,
                    request.job_name,
                    request.metadata.get("job_type", "unknown"),
                    item.get("url", ""),
                    json.dumps(item),
                    json.dumps(item),  # Could be enhanced processing
                    data_type,
                    content_hash,
                    item.get("timestamp", datetime.utcnow().isoformat()),
                    quality_score,
                    completeness_score,
                    "valid" if quality_score >= 70 else "pending",
                    word_count,
                    link_count,
                    image_count,
                    crawl_metadata,
                ),
            )

            centralized_count += 1

        conn.commit()
        conn.close()

        return {
            "status": "success",
            "message": f"Successfully centralized {centralized_count} records",
            "centralized_records": centralized_count,
            "duplicates_found": duplicate_count,
            "total_processed": len(request.data),
        }

    except Exception as e:
        print(f"Error centralizing data: {e}")
        return {"status": "error", "message": f"Failed to centralize data: {str(e)}"}


@app.get("/api/data/consolidate")
async def consolidate_all_data(current_user: dict = Depends(get_current_user)):
    """
    Consolidate all job data into centralized database
    """
    try:
        # This could be enhanced to automatically process all completed jobs
        return {
            "status": "success",
            "message": "Data consolidation completed successfully",
        }
    except Exception as e:
        return {"status": "error", "message": f"Failed to consolidate data: {str(e)}"}


if __name__ == "__main__":
    print("ðŸš€ Starting Business Intelligence Scraper API Server...")
    current_config = get_config()
    frontend_url = getattr(current_config, 'FRONTEND_URL', 'http://localhost:3000')
    api_docs_url = getattr(current_config, 'API_DOCS_URL', 'http://localhost:8000/docs')
    print(f"ðŸ“Š Dashboard: {frontend_url}")
    print(f"ðŸ”— API Docs: {api_docs_url}")
    print("ðŸ’¾ Database: SQLite at", DATABASE_PATH)
    print("ðŸ” Security Features:")
    print(f"   âœ… Rate Limiting: {security_config.API_RATE_LIMIT_PER_MINUTE}/min")
    print(f"   âœ… Security Headers: {security_config.ENABLE_SECURITY_HEADERS}")
    print("   âœ… Input Validation: Enabled")
    print(f"   âœ… Secure JWT: {len(security_config.JWT_SECRET)} char secret")
    print("âš¡ Performance Features:")
    print(f"   âœ… Performance Monitoring: {PERFORMANCE_ENABLED}")
    print(f"   âœ… Caching System: {PERFORMANCE_ENABLED}")
    print(f"   âœ… Database Optimization: {PERFORMANCE_ENABLED}")
    print(f"   âœ… Real-time Metrics: {PERFORMANCE_ENABLED}")


# ==========================================
# DATABASE MANAGEMENT ENDPOINTS
# ==========================================


@app.get("/api/database/tables")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def get_database_tables(
    request: Request, current_user: dict = Depends(get_current_user)
):
    """Get list of all database tables and their info"""
    try:
        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        # Get table names
        cursor.execute(
            "SELECT name FROM sqlite_master WHERE type='table' ORDER BY name"
        )
        tables = []

        for (table_name,) in cursor.fetchall():
            # Get table info
            cursor.execute(f"PRAGMA table_info({table_name})")
            columns = cursor.fetchall()

            # Get row count
            cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
            row_count = cursor.fetchone()[0]

            tables.append(
                {
                    "name": table_name,
                    "columns": [
                        {
                            "name": col[1],
                            "type": col[2],
                            "nullable": not col[3],
                            "primary_key": bool(col[5]),
                        }
                        for col in columns
                    ],
                    "row_count": row_count,
                }
            )

        conn.close()
        return {"tables": tables}

    except Exception as e:
        logger.error(f"Error getting database tables: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@app.get("/api/database/table/{table_name}")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def get_table_data(
    table_name: str,
    request: Request,
    limit: int = 100,
    offset: int = 0,
    current_user: dict = Depends(get_current_user),
):
    """Get data from a specific table with pagination"""
    try:
        # Validate table name to prevent SQL injection
        valid_tables = ["users", "jobs", "job_results", "analytics"]
        if table_name not in valid_tables:
            raise HTTPException(status_code=400, detail="Invalid table name")

        conn = sqlite3.connect(DATABASE_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        # Get total count
        cursor.execute(f"SELECT COUNT(*) FROM {table_name}")
        total_count = cursor.fetchone()[0]

        # Get paginated data
        cursor.execute(
            f"SELECT * FROM {table_name} ORDER BY id DESC LIMIT ? OFFSET ?",
            (limit, offset),
        )
        rows = [dict(row) for row in cursor.fetchall()]

        conn.close()

        return {
            "table_name": table_name,
            "total_count": total_count,
            "limit": limit,
            "offset": offset,
            "data": rows,
        }

    except Exception as e:
        logger.error(f"Error getting table data: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@app.post("/api/database/query")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def execute_database_query(
    request: Request, query_data: dict, current_user: dict = Depends(get_current_user)
):
    """Execute a custom SQL query (read-only for safety)"""
    try:
        query = query_data.get("query", "").strip()
        if not query:
            raise HTTPException(status_code=400, detail="Query is required")

        # Only allow SELECT queries for safety
        if not query.upper().startswith("SELECT"):
            raise HTTPException(
                status_code=400, detail="Only SELECT queries are allowed"
            )

        conn = sqlite3.connect(DATABASE_PATH)
        conn.row_factory = sqlite3.Row
        cursor = conn.cursor()

        cursor.execute(query)
        rows = [dict(row) for row in cursor.fetchall()]

        conn.close()

        return {"query": query, "result_count": len(rows), "data": rows}

    except sqlite3.Error as e:
        logger.error(f"SQL error: {str(e)}")
        raise HTTPException(status_code=400, detail=f"SQL error: {str(e)}")
    except Exception as e:
        logger.error(f"Error executing query: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@app.delete("/api/database/table/{table_name}/record/{record_id}")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def delete_record(
    table_name: str,
    record_id: int,
    request: Request,
    current_user: dict = Depends(get_current_user),
):
    """Delete a specific record from a table"""
    try:
        # Validate table name
        valid_tables = [
            "jobs",
            "job_results",
            "analytics",
        ]  # Don't allow deleting users
        if table_name not in valid_tables:
            raise HTTPException(status_code=400, detail="Cannot delete from this table")

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        # Check if record exists
        cursor.execute(f"SELECT COUNT(*) FROM {table_name} WHERE id = ?", (record_id,))
        if cursor.fetchone()[0] == 0:
            raise HTTPException(status_code=404, detail="Record not found")

        # Delete the record
        cursor.execute(f"DELETE FROM {table_name} WHERE id = ?", (record_id,))
        conn.commit()
        conn.close()

        return {"message": f"Record {record_id} deleted from {table_name}"}

    except Exception as e:
        logger.error(f"Error deleting record: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


@app.post("/api/database/cleanup")
@limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
async def cleanup_database(
    request: Request, cleanup_data: dict, current_user: dict = Depends(get_current_user)
):
    """Clean up database by removing old or incomplete records"""
    try:
        cleanup_type = cleanup_data.get("type", "")

        conn = sqlite3.connect(DATABASE_PATH)
        cursor = conn.cursor()

        deleted_count = 0

        if cleanup_type == "failed_jobs":
            # Delete failed jobs and their results
            cursor.execute(
                "DELETE FROM job_results WHERE job_id IN (SELECT id FROM jobs WHERE status = 'failed')"
            )
            cursor.execute("DELETE FROM jobs WHERE status = 'failed'")
            deleted_count = cursor.rowcount

        elif cleanup_type == "old_analytics":
            # Delete analytics older than 30 days
            cursor.execute(
                "DELETE FROM analytics WHERE timestamp < datetime('now', '-30 days')"
            )
            deleted_count = cursor.rowcount

        elif cleanup_type == "empty_results":
            # Delete job results with no data
            cursor.execute("DELETE FROM job_results WHERE data IS NULL OR data = ''")
            deleted_count = cursor.rowcount

        else:
            raise HTTPException(status_code=400, detail="Invalid cleanup type")

        conn.commit()
        conn.close()

        return {"message": f"Cleanup completed", "deleted_count": deleted_count}

    except Exception as e:
        logger.error(f"Error cleaning up database: {str(e)}")
        raise HTTPException(status_code=500, detail=f"Database error: {str(e)}")


# ========================
# AI INTEGRATION ENDPOINTS (Phase 4)
# ========================

if AI_INTEGRATION_AVAILABLE:

    @app.post("/api/ai/analyze")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def analyze_data_with_ai(
        request_data: dict,
        request: Request,
        current_user: dict = Depends(get_current_user),
    ):
        """Analyze scraped data with AI insights"""
        try:
            data = request_data.get("data", [])
            analysis_type = request_data.get("analysis_type", "full")
            options = request_data.get("options", {})

            if not data:
                raise HTTPException(
                    status_code=400, detail="No data provided for analysis"
                )

            logger.info(f"ðŸ¤– Starting AI analysis for {len(data)} data points")

            if not AI_INTEGRATION_AVAILABLE or 'ai_service' not in globals():
                raise HTTPException(
                    status_code=503, detail="AI service not available"
                )

            # Perform AI analysis
            ai_svc = get_ai_service()
            result = await ai_svc.analyze_scraped_data(data, analysis_type, options)

            return {
                "analysis_id": result.request_id,
                "insights": result.insights,
                "visualizations": result.visualizations,
                "recommendations": result.recommendations,
                "processing_time": result.processing_time,
                "timestamp": result.timestamp.isoformat(),
                "data_count": len(data),
            }

        except Exception as e:
            logging.error(f"âŒ AI analysis error: {e}")
            raise HTTPException(status_code=500, detail=f"AI analysis failed: {str(e)}")

    @app.get("/api/ai/realtime-dashboard")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def get_realtime_ai_dashboard(
        request: Request, current_user: dict = Depends(get_current_user)
    ):
        """Get real-time AI analytics dashboard data"""
        try:
            ai_svc = get_ai_service()
            dashboard_data = ai_svc.get_realtime_dashboard_data()

            return {
                "dashboard": dashboard_data,
                "ai_service_stats": ai_svc.get_service_statistics(),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logging.error(f"âŒ Real-time dashboard error: {e}")
            raise HTTPException(status_code=500, detail=f"Dashboard error: {str(e)}")

    @app.post("/api/ai/recommendations")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def get_ai_recommendations(
        request_data: dict,
        request: Request,
        current_user: dict = Depends(get_current_user),
    ):
        """Get AI-powered recommendations for data improvement"""
        try:
            data = request_data.get("data", [])

            if not data:
                raise HTTPException(
                    status_code=400, detail="No data provided for recommendations"
                )

            ai_svc = get_ai_service()
            recommendations = await ai_svc.generate_ai_recommendations(data)

            return {
                "recommendations": recommendations,
                "data_analyzed": len(data),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logging.error(f"âŒ AI recommendations error: {e}")
            raise HTTPException(
                status_code=500, detail=f"Recommendations failed: {str(e)}"
            )

    @app.post("/api/ai/optimize-strategy")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def optimize_scraping_strategy(
        request_data: dict,
        request: Request,
        current_user: dict = Depends(get_current_user),
    ):
        """Get AI-powered scraping strategy optimization"""
        try:
            data = request_data.get("data", [])

            if not data:
                raise HTTPException(
                    status_code=400, detail="No data provided for optimization"
                )

            ai_svc = get_ai_service()
            optimization = await ai_svc.optimize_scraping_strategy(data)

            return {
                "optimization_strategy": optimization,
                "data_analyzed": len(data),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logging.error(f"âŒ Strategy optimization error: {e}")
            raise HTTPException(
                status_code=500, detail=f"Optimization failed: {str(e)}"
            )

    @app.get("/api/ai/analysis/{analysis_id}")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def get_analysis_result(
        analysis_id: str,
        request: Request,
        current_user: dict = Depends(get_current_user),
    ):
        """Get analysis result by ID"""
        try:
            ai_svc = get_ai_service()
            result = ai_svc.get_analysis_result(analysis_id)

            if not result:
                raise HTTPException(status_code=404, detail="Analysis result not found")

            return {
                "analysis_id": result.request_id,
                "insights": result.insights,
                "visualizations": result.visualizations,
                "recommendations": result.recommendations,
                "processing_time": result.processing_time,
                "timestamp": result.timestamp.isoformat(),
            }

        except HTTPException:
            raise
        except Exception as e:
            logging.error(f"âŒ Get analysis result error: {e}")
            raise HTTPException(
                status_code=500, detail=f"Failed to get analysis: {str(e)}"
            )

    @app.post("/api/ai/queue-analysis")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def queue_ai_analysis(
        request_data: dict,
        request: Request,
        current_user: dict = Depends(get_current_user),
    ):
        """Queue AI analysis for background processing"""
        try:
            data = request_data.get("data", [])
            analysis_type = request_data.get("analysis_type", "full")
            options = request_data.get("options", {})

            if not data:
                raise HTTPException(
                    status_code=400, detail="No data provided for analysis"
                )

            ai_svc = get_ai_service()
            analysis_id = await ai_svc.queue_analysis(data, analysis_type, options)

            return {
                "analysis_id": analysis_id,
                "status": "queued",
                "message": "Analysis queued for background processing",
                "data_count": len(data),
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logging.error(f"âŒ Queue analysis error: {e}")
            raise HTTPException(
                status_code=500, detail=f"Failed to queue analysis: {str(e)}"
            )

    @app.get("/api/ai/service/status")
    @limiter.limit(f"{security_config.API_RATE_LIMIT_PER_MINUTE}/minute")
    async def get_ai_service_status(
        request: Request, current_user: dict = Depends(get_current_user)
    ):
        """Get AI service status and statistics"""
        try:
            ai_svc = get_ai_service()
            return {
                "ai_service_available": True,
                "service_statistics": ai_svc.get_service_statistics(),
                "capabilities": {
                    "content_clustering": True,
                    "predictive_analytics": True,
                    "real_time_monitoring": True,
                    "visualization_generation": True,
                    "ai_recommendations": True,
                    "strategy_optimization": True,
                },
                "timestamp": datetime.now().isoformat(),
            }

        except Exception as e:
            logging.error(f"âŒ AI service status error: {e}")
            raise HTTPException(
                status_code=500, detail=f"Service status error: {str(e)}"
            )

else:

    @app.get("/api/ai/service/status")
    async def ai_service_unavailable(
        request: Request, current_user: dict = Depends(get_current_user)
    ):
        """AI service unavailable response"""
        return {
            "ai_service_available": False,
            "message": "AI Integration Service is not available",
            "timestamp": datetime.now().isoformat(),
        }


if __name__ == "__main__":
    uvicorn.run(
        "backend_server:app",
        host=server_config.HOST,
        port=server_config.PORT,
        reload=server_config.DEBUG,
        log_level=server_config.LOG_LEVEL,
    )
