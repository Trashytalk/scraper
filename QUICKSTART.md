# ğŸš€ Quick Start Guide

Get the Business Intelligence Scraper running in under 5 minutes!

## âš¡ **NEW: One-Command Setup**

```bash
# Clone and start instantly with our comprehensive quick start script
git clone https://github.com/Trashytalk/scraper.git
cd scraper
./quick_start.sh
```

**This single command will:**
- âœ… Check system requirements (Python 3.8+)
- âœ… Set up virtual environment automatically
- âœ… Install all dependencies
- âœ… Configure the application
- âœ… Initialize database
- âœ… Start Redis (via Docker if needed)
- âœ… Launch web server on port 8000
- âœ… Show access information and examples

**Additional Quick Start Options:**
```bash
./quick_start.sh --help       # Show all options
./quick_start.sh --dev        # Start in development mode
./quick_start.sh --status     # Check service status
./quick_start.sh --stop       # Stop all services
./quick_start.sh --clean      # Clean and reset
```

## âš¡ One-Command Setup

```bash
# Clone and setup everything automatically
git clone https://github.com/Trashytalk/scraper.git
cd scraper
./setup.sh
```

## ğŸ¯ 5-Minute Demo

```bash
# Run the interactive demo
./demo.sh
```

This will:

- âœ… Start Redis automatically  
- âœ… Launch the API server
- âœ… Run an example scraping job
- âœ… Show you where to access the results

## ğŸŒ Access Points

Once running, you can access:

- **ğŸ”— API Health Check**: <http://localhost:8000/>
- **ğŸ“š Interactive API Docs**: <http://localhost:8000/docs>
- **ğŸ“Š GraphQL Playground**: <http://localhost:8000/graphql>
- **ğŸ“ˆ Metrics**: <http://localhost:8000/metrics>

## ğŸ“± Quick API Examples

### Start a Scraping Job
```bash
curl -X POST http://localhost:8000/scrape
# Returns: {"task_id": "abc123..."}
```

### Check Job Status
```bash
curl http://localhost:8000/tasks/abc123
# Returns: {"status": "completed", "result": {...}}
```

### View All Data
```bash
curl http://localhost:8000/data
# Returns: [{"title": "Example Company", "url": "..."}]
```

### Export as CSV
```bash
curl "http://localhost:8000/export?format=csv" > results.csv
```

## ğŸ› ï¸ Development Setup

```bash
# Setup with development tools
./setup.sh --dev

# Activate environment
source .venv/bin/activate

# Start with auto-reload
cd business_intel_scraper
uvicorn backend.api.main:app --reload
```

## ğŸ›ï¸ Frontend Dashboard

```bash
# Install frontend (optional)
cd business_intel_scraper/frontend
npm install
npm start

# Access dashboard at http://localhost:8000
```

## ğŸ†˜ Need Help?

- **ğŸ“– Full Documentation**: [docs/README.md](docs/README.md)
- **ğŸ—ï¸ Architecture**: [docs/architecture.md](docs/architecture.md) 
- **âš™ï¸ Configuration**: [docs/setup.md](docs/setup.md)
- **ğŸ”’ Security**: [docs/security.md](docs/security.md)

## ğŸª What's Included?

- âœ… **RESTful API** with FastAPI
- âœ… **GraphQL endpoint** for flexible queries
- âœ… **Real-time WebSocket** notifications
- âœ… **Celery task queue** for async processing
- âœ… **Multiple scraping engines** (Scrapy, Playwright)
- âœ… **OSINT integrations** (SpiderFoot, theHarvester, etc.)
- âœ… **Database models** with migrations
- âœ… **Authentication & authorization**
- âœ… **Rate limiting & security**
- âœ… **Prometheus metrics**
- âœ… **Docker containerization**

## ğŸ§ª Run Tests

```bash
# Run comprehensive test coverage
python3 tests/run_full_coverage.py --parallel --coverage --save-reports

# Run specific test suites
python3 tests/run_full_coverage.py --suites root_modules gui_components

# Run legacy test commands
python -m pytest tests/ -v --cov=.
```

## ğŸ”§ Troubleshooting

**Port 8000 already in use?**
```bash
# Find and kill the process
lsof -ti:8000 | xargs kill -9
```

**Redis connection failed?**
```bash
# Start Redis manually
docker run -d -p 6379:6379 --name redis redis:7
```

**Permission denied on setup.sh?**
```bash
chmod +x setup.sh
```

---

ğŸ‰ **You're ready to scrape!** Check out the [tutorials](docs/tutorial.md) for more advanced usage.
