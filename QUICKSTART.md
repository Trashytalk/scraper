# ğŸš€ Quick Start Guide - Business Intelligence Scraper

**Get your Business Intelligence Platform running in 2-3 minutes with our automated setup script!**

## ğŸ¯ **One-Command Setup (Recommended)**

**Step 1: Clone and Navigate**

```bash

git clone https://github.com/Trashytalk/scraper.git
cd scraper

```

**Step 2: Make Script Executable (First Time Only)**

```bash

chmod +x quick_start.sh

```

**Step 3: Start Everything**

```bash

./quick_start.sh

```

### âœ¨ **What the Quick Start Script Does Automatically**

The `quick_start.sh` script provides a comprehensive automated setup:

- âœ… **System Requirements Check**: Verifies Python 3.8+, pip, and essential dependencies
- âœ… **Environment Setup**: Creates isolated Python virtual environment
- âœ… **Dependency Installation**: Installs all required packages (2-3 minutes)
- âœ… **Database Initialization**: Sets up SQLite database with proper schemas
- âœ… **Redis Server Management**: Starts Redis for caching and session management
- âœ… **Web Server Launch**: Starts FastAPI backend server on port 8000
- âœ… **Health Verification**: Checks all services are running correctly
- âœ… **Access Information**: Provides URLs and login credentials

### ğŸ‰ **Expected Output**

When you run `./quick_start.sh`, you'll see:

```bash

ğŸš€ Business Intelligence Scraper - Quick Start
==============================================

âœ… Checking system requirements...
âœ… Setting up Python virtual environment...
âœ… Installing dependencies (this may take 2-3 minutes)...
âœ… Initializing database...
âœ… Starting Redis server...
âœ… Starting web server...

ğŸ‰ Setup Complete!

ğŸ“Š Dashboard: http://localhost:8000
ğŸ“– API Docs: http://localhost:8000/docs
ğŸ“ˆ Admin Panel: http://localhost:8000/admin

ğŸ” Default Login Credentials:
   Username: admin
   Password: admin123

Press Ctrl+C to stop all services

```

## ğŸ”§ **Advanced Quick Start Options**

The quick start script supports multiple modes for different use cases:

### **Development Mode**

```bash

./quick_start.sh --dev

```
- Enables hot reload for code changes
- Provides detailed debugging output
- Uses development configuration

### **Production Mode**

```bash

./quick_start.sh --production

```
- Optimized for production deployment
- Enhanced security settings
- Performance optimizations enabled

### **Clean Installation**

```bash

./quick_start.sh --clean

```
- Removes existing virtual environment
- Fresh installation of all dependencies
- Useful for troubleshooting

### **System Status Check**

```bash

./quick_start.sh --status

```
- Checks if services are running
- Shows port usage and process information
- Validates system health

### **Stop Services**

```bash

./quick_start.sh --stop

```
- Gracefully stops all running services
- Cleans up background processes
- Preserves data and configuration

### **Help and Options**

```bash

./quick_start.sh --help

```
- Shows all available options
- Provides usage examples
- Displays troubleshooting tips

## âš¡ One-Command Setup

```bash

# Clone and setup everything automatically

git clone https://github.com/Trashytalk/scraper.git
cd scraper
./setup.sh

```

## ğŸ¯ 5-Minute Demo

```bash

# Run the interactive demo

./demo.sh

```

This will:

- âœ… Start Redis automatically
- âœ… Launch the API server
- âœ… Run an example scraping job
- âœ… Show you where to access the results

## ğŸŒ Access Points

Once running, you can access:

- **ğŸ”— API Health Check**: <http://localhost:8000/>
- **ğŸ“š Interactive API Docs**: <http://localhost:8000/docs>
- **ğŸ“Š GraphQL Playground**: <http://localhost:8000/graphql>
- **ğŸ“ˆ Metrics**: <http://localhost:8000/metrics>

## ğŸ“± Quick API Examples

### Start a Scraping Job

```bash

curl -X POST http://localhost:8000/scrape

# Returns: {"task_id": "abc123..."}

```

### Check Job Status

```bash

curl http://localhost:8000/tasks/abc123

# Returns: {"status": "completed", "result": {...}}

```

### View All Data

```bash

curl http://localhost:8000/data

# Returns: [{"title": "Example Company", "url": "..."}]

```

### Export as CSV

```bash

curl "http://localhost:8000/export?format=csv" > results.csv

```

## ğŸ› ï¸ Development Setup

```bash

# Setup with development tools

./setup.sh --dev

# Activate environment

source .venv/bin/activate

# Start with auto-reload

cd business_intel_scraper
uvicorn backend.api.main:app --reload

```

## ğŸ›ï¸ Frontend Dashboard

```bash

# Install frontend (optional)

cd business_intel_scraper/frontend
npm install
npm start

# Access dashboard at http://localhost:8000

```

## ğŸ†˜ Need Help?

- **ğŸ“– Full Documentation**: [docs/README.md](docs/README.md)
- **ğŸ—ï¸ Architecture**: [docs/architecture.md](docs/architecture.md)
- **âš™ï¸ Configuration**: [docs/setup.md](docs/setup.md)
- **ğŸ”’ Security**: [docs/security.md](docs/security.md)

## ğŸª What's Included?

- âœ… **RESTful API** with FastAPI
- âœ… **GraphQL endpoint** for flexible queries
- âœ… **Real-time WebSocket** notifications
- âœ… **Celery task queue** for async processing
- âœ… **Multiple scraping engines** (Scrapy, Playwright)
- âœ… **OSINT integrations** (SpiderFoot, theHarvester, etc.)
- âœ… **Database models** with migrations
- âœ… **Authentication & authorization**
- âœ… **Rate limiting & security**
- âœ… **Prometheus metrics**
- âœ… **Docker containerization**

## ğŸ§ª Run Tests

```bash

# Run comprehensive test coverage

python3 tests/run_full_coverage.py --parallel --coverage --save-reports

# Run specific test suites

python3 tests/run_full_coverage.py --suites root_modules gui_components

# Run legacy test commands

python -m pytest tests/ -v --cov=.

```

## ğŸ”§ Troubleshooting

**Port 8000 already in use?**

```bash

# Find and kill the process

lsof -ti:8000 | xargs kill -9

```

**Redis connection failed?**

```bash

# Start Redis manually

docker run -d -p 6379:6379 --name redis redis:7

```

**Permission denied on setup.sh?**

```bash

chmod +x setup.sh

```


---


ğŸ‰ **You're ready to scrape!** Check out the [tutorials](docs/tutorial.md) for more advanced usage.
