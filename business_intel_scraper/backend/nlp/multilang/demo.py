#!/usr/bin/env python3
"""
Multi-Language NLP Integration Demo

Demonstrates the complete multi-language NLP system for business intelligence.
Run this script to test all components with sample data.
"""

import asyncio
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def print_section(title: str):
    """Print formatted section header"""
    print(f"\n{'='*60}")
    print(f"  {title}")
    print("=" * 60)


def print_subsection(title: str):
    """Print formatted subsection header"""
    print(f"\n{'-'*40}")
    print(f"  {title}")
    print("-" * 40)


def main():
    """Main demonstration function"""
    print_section("Multi-Language NLP System Demo")

    # Sample texts in different languages
    sample_texts = {
        "English": "Apple Inc. is located at 1 Apple Park Way, Cupertino, CA 95014, USA. Revenue: $365.8B. Contact: +1-408-996-1010.",
        "Chinese": "è‹¹æœå…¬å¸ä½äºç¾å›½åŠ åˆ©ç¦å°¼äºšå·åº“æ¯”è’‚è¯ºè‹¹æœå…¬å›­å¤§é“1å·ï¼Œé‚®ç¼–95014ã€‚è¥æ”¶3658äº¿ç¾å…ƒã€‚ç”µè¯ï¼š+1-408-996-1010ã€‚",
        "Russian": "Ğ¯Ğ½Ğ´ĞµĞºÑ ÑĞ²Ğ»ÑĞµÑ‚ÑÑ Ñ€Ğ¾ÑÑĞ¸Ğ¹ÑĞºĞ¾Ğ¹ ĞºĞ¾Ğ¼Ğ¿Ğ°Ğ½Ğ¸ĞµĞ¹. ĞĞ´Ñ€ĞµÑ: ĞœĞ¾ÑĞºĞ²Ğ°, ÑƒĞ». Ğ›ÑŒĞ²Ğ° Ğ¢Ğ¾Ğ»ÑÑ‚Ğ¾Ğ³Ğ¾, Ğ´. 16. Ğ”Ğ¾Ñ…Ğ¾Ğ´: 280 Ğ¼Ğ»Ñ€Ğ´ Ñ€ÑƒĞ±. Ğ¢ĞµĞ»ĞµÑ„Ğ¾Ğ½: +7-495-739-70-00.",
        "Arabic": "Ø£Ø±Ø§Ù…ÙƒÙˆ Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ© Ø´Ø±ÙƒØ© Ù†ÙØ· Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø§Ù„Ø¹Ù†ÙˆØ§Ù†: Ø§Ù„Ø¸Ù‡Ø±Ø§Ù†ØŒ Ø§Ù„Ù…Ù…Ù„ÙƒØ© Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„Ø³Ø¹ÙˆØ¯ÙŠØ©. Ø§Ù„Ø¯Ø®Ù„: 400 Ù…Ù„ÙŠØ§Ø± Ø¯ÙˆÙ„Ø§Ø±. Ø§Ù„Ù‡Ø§ØªÙ: +966-13-872-7000.",
        "Japanese": "ãƒˆãƒ¨ã‚¿è‡ªå‹•è»Šæ ªå¼ä¼šç¤¾ã¯æ—¥æœ¬ã®è‡ªå‹•è»Šãƒ¡ãƒ¼ã‚«ãƒ¼ã§ã™ã€‚ä½æ‰€ï¼šæ„›çŸ¥çœŒè±Šç”°å¸‚ãƒˆãƒ¨ã‚¿ç”º1ç•ªåœ°ã€‚å£²ä¸Šï¼š30å…†å††ã€‚é›»è©±ï¼š+81-565-28-2121ã€‚",
        "Spanish": "Banco Santander tiene sede en Madrid, EspaÃ±a. DirecciÃ³n: Paseo de la Castellana 24. Ingresos: â‚¬50.2B. TelÃ©fono: +34-91-289-0000.",
        "German": "SAP SE hat ihren Sitz in Weinheim, Deutschland. Adresse: Dietmar-Hopp-Allee 16. Umsatz: â‚¬27.8B. Telefon: +49-6227-7-47474.",
        "French": "Total SE est une compagnie franÃ§aise. Adresse: 2 Place Jean Millier, La DÃ©fense. Chiffre d'affaires: â‚¬140B. TÃ©lÃ©phone: +33-1-47-44-45-46.",
    }

    try:
        # Import the multi-language processor
        try:
            from business_intel_scraper.backend.nlp.multilang import multilang_processor

            HAS_MULTILANG = True
        except ImportError as e:
            logger.warning(f"Multi-language processor not available: {e}")
            HAS_MULTILANG = False

        if not HAS_MULTILANG:
            print("âŒ Multi-language processor not available")
            print("Please install dependencies: pip install -r requirements.txt")
            return

        # Test 1: Language Detection
        print_section("1. Language Detection Test")

        for lang_name, text in sample_texts.items():
            try:
                detected = multilang_processor.detector.create_detected_text(text)
                print(
                    f"ğŸ” {lang_name:10} â†’ {detected.language.name:15} "
                    f"({detected.language.code}) "
                    f"Script: {detected.script.value:10} "
                    f"Confidence: {detected.confidence:.2f}"
                )
            except Exception as e:
                print(f"âŒ {lang_name:10} â†’ Error: {e}")

        # Test 2: Complete Processing
        print_section("2. Complete Multi-Language Processing")

        test_text = sample_texts["English"]  # Start with English
        print(f"Processing: {test_text[:80]}...")

        try:
            result = multilang_processor.process_text(
                test_text,
                include_transliteration=True,
                include_translation=False,  # Skip translation for demo
                include_normalization=True,
            )

            print(
                f"âœ… Language: {result.detected_language.language.name} ({result.detected_language.confidence:.2f})"
            )
            print(f"âœ… Script: {result.detected_language.script.value}")

            if result.tokenization:
                print(f"âœ… Tokens: {len(result.tokenization.tokens)} tokens")
                print(f"   Sample: {result.tokenization.tokens[:5]}...")

            if result.entities:
                print(f"âœ… Entities: {len(result.entities)} found")
                for entity in result.entities[:3]:  # Show first 3
                    entity_type = (
                        entity.entity_type.value
                        if hasattr(entity.entity_type, "value")
                        else str(entity.entity_type)
                    )
                    print(
                        f"   - {entity_type}: '{entity.text}' (conf: {entity.confidence:.2f})"
                    )

            if result.normalized_entities:
                print(
                    f"âœ… Normalized entities: {len(result.normalized_entities)} types"
                )
                for entity_type, norm_list in result.normalized_entities.items():
                    print(f"   - {entity_type}: {len(norm_list)} items")

        except Exception as e:
            print(f"âŒ Processing failed: {e}")

        # Test 3: Business Intelligence Extraction
        print_section("3. Business Intelligence Extraction")

        try:
            intelligence = multilang_processor.extract_business_intelligence(test_text)

            print("ğŸ“Š Language Information:")
            lang_info = intelligence["language_info"]
            print(
                f"   Language: {lang_info['detected_language']} ({lang_info['language_code']})"
            )
            print(f"   Script: {lang_info['script']}")
            print(f"   Confidence: {lang_info['confidence']:.2f}")

            print("ğŸ“Š Extracted Entities:")
            entities = intelligence["entities"]
            for entity_type, entity_list in entities.items():
                print(f"   {entity_type}: {len(entity_list)} items")
                for entity in entity_list[:2]:  # Show first 2 of each type
                    print(
                        f"     - '{entity['text']}' (conf: {entity['confidence']:.2f})"
                    )

            print("ğŸ“Š Structured Data:")
            structured = intelligence["structured_data"]
            for data_type, data_list in structured.items():
                print(f"   {data_type}: {len(data_list)} items")
                for item in data_list[:1]:  # Show first item of each type
                    print(f"     - Original: '{item['original']}'")
                    print(f"     - Normalized: '{item['normalized']}'")

        except Exception as e:
            print(f"âŒ Business intelligence extraction failed: {e}")

        # Test 4: Normalization Demo
        print_section("4. Field Normalization Demo")

        normalization_tests = [
            ("Phone", "+1-408-996-1010", "phone"),
            ("Phone", "(555) 123-4567", "phone"),
            ("Amount", "$365.8 billion", "financial"),
            ("Amount", "â‚¬1,234.56", "financial"),
            ("Address", "1 Apple Park Way, Cupertino, CA 95014", "address"),
            ("Company ID", "12-3456789", "company_id"),
            ("Date", "2023-12-25", "date"),
        ]

        for test_name, test_value, field_type in normalization_tests:
            try:
                if field_type == "phone":
                    from business_intel_scraper.backend.nlp.multilang.normalization import (
                        phone_normalizer,
                    )

                    result = phone_normalizer.normalize_phone(test_value)
                elif field_type == "financial":
                    from business_intel_scraper.backend.nlp.multilang.normalization import (
                        financial_normalizer,
                    )

                    result = financial_normalizer.normalize_amount(test_value)
                elif field_type == "address":
                    from business_intel_scraper.backend.nlp.multilang.normalization import (
                        address_normalizer,
                    )

                    lang = multilang_processor.detector.language_data["en"]
                    result = address_normalizer.normalize_address(test_value, lang)
                elif field_type == "company_id":
                    from business_intel_scraper.backend.nlp.multilang.normalization import (
                        company_id_normalizer,
                    )

                    result = company_id_normalizer.normalize_company_id(test_value)
                elif field_type == "date":
                    from business_intel_scraper.backend.nlp.multilang.normalization import (
                        date_normalizer,
                    )

                    lang = multilang_processor.detector.language_data["en"]
                    result = date_normalizer.normalize_date(test_value, lang)
                else:
                    continue

                print(
                    f"ğŸ”§ {test_name:12} '{test_value}' â†’ '{result.normalized}' "
                    f"(conf: {result.confidence:.2f})"
                )

            except Exception as e:
                print(f"âŒ {test_name:12} normalization failed: {e}")

        # Test 5: Cross-Language Entity Matching
        print_section("5. Cross-Language Entity Matching")

        try:
            from business_intel_scraper.backend.nlp.multilang.transliteration import (
                entity_normalizer,
            )

            # Test company name matching across languages
            companies = [
                ("Apple Inc.", multilang_processor.detector.language_data["en"]),
                ("è‹¹æœå…¬å¸", multilang_processor.detector.language_data["zh"]),
                (
                    "Samsung Electronics",
                    multilang_processor.detector.language_data["en"],
                ),
                (
                    "ì‚¼ì„±ì „ì",
                    multilang_processor.detector.language_data.get(
                        "ko", multilang_processor.detector.language_data["en"]
                    ),
                ),
            ]

            normalized_companies = []
            for company_name, language in companies:
                try:
                    normalized = entity_normalizer.normalize_company_name(
                        company_name, language
                    )
                    normalized_companies.append((company_name, normalized))

                    print(f"ğŸ¢ {company_name:20} ({language.code}):")
                    print(f"   Original: {normalized.get('original', 'N/A')}")
                    print(f"   Cleaned: {normalized.get('cleaned', 'N/A')}")
                    if "transliterated" in normalized:
                        print(f"   Transliterated: {normalized['transliterated']}")
                    if "translated" in normalized:
                        print(f"   Translated: {normalized['translated']}")

                except Exception as e:
                    print(f"âŒ Failed to normalize {company_name}: {e}")

            # Calculate similarities
            if len(normalized_companies) >= 2:
                print("\nğŸ”— Similarity Scores:")
                for i in range(len(normalized_companies)):
                    for j in range(i + 1, len(normalized_companies)):
                        try:
                            name1, norm1 = normalized_companies[i]
                            name2, norm2 = normalized_companies[j]
                            similarity = entity_normalizer.calculate_similarity(
                                norm1, norm2
                            )
                            print(f"   {name1} â†” {name2}: {similarity:.3f}")
                        except Exception as e:
                            print(f"âŒ Similarity calculation failed: {e}")

        except Exception as e:
            print(f"âŒ Cross-language matching failed: {e}")

        # Test 6: Batch Processing
        print_section("6. Batch Processing Demo")

        try:
            # Process multiple texts
            batch_texts = list(sample_texts.values())[:3]  # First 3 texts
            print(f"Processing {len(batch_texts)} texts in batch...")

            results = multilang_processor.batch_process(
                batch_texts,
                include_translation=False,
                include_normalization=False,  # Skip for faster demo
            )

            for i, result in enumerate(results):
                if hasattr(result, "detected_language"):
                    print(
                        f"ğŸ“ Text {i+1}: {result.detected_language.language.name} "
                        f"({len(result.entities)} entities)"
                    )
                else:
                    print(f"âŒ Text {i+1}: Processing failed")

        except Exception as e:
            print(f"âŒ Batch processing failed: {e}")

        # Test 7: Capabilities Report
        print_section("7. System Capabilities")

        try:
            capabilities = multilang_processor.get_capabilities()

            print(f"ğŸŒ Supported Languages: {len(capabilities['languages'])}")
            print(f"   Languages: {', '.join(capabilities['languages'][:10])}...")

            print(f"ğŸ“ Scripts: {', '.join(capabilities['scripts'])}")

            tokenization = capabilities.get("tokenization_support", {})
            print(f"ğŸ”¤ Tokenization: {len(tokenization)} language-specific tokenizers")

            ner = capabilities.get("ner_support", {})
            print("ğŸ·ï¸  NER Support:")
            if "spacy" in ner:
                print(f"   spaCy: {len(ner['spacy'])} languages")
            if "transformers" in ner:
                print(f"   Transformers: {', '.join(ner['transformers'])}")

            transliteration = capabilities.get("transliteration_support", {})
            if transliteration.get("icu_available"):
                print("ğŸ”¤ Transliteration: ICU available")
            print(
                f"   Supported scripts: {', '.join(transliteration.get('supported_scripts', []))}"
            )

            translation = capabilities.get("translation_support", {})
            if translation.get("google_available"):
                print("ğŸŒ Translation: Google Translate available")
            marian_models = translation.get("marian_models", [])
            if marian_models:
                print(f"   Offline models: {len(marian_models)} language pairs")

            normalization = capabilities.get("normalization_support", {})
            supported_fields = [
                field for field, available in normalization.items() if available
            ]
            print(f"ğŸ”§ Normalization: {', '.join(supported_fields)}")

        except Exception as e:
            print(f"âŒ Capabilities report failed: {e}")

        # Summary
        print_section("Demo Complete")
        print("âœ… Multi-Language NLP system demonstration completed!")
        print("\nğŸ“‹ Summary:")
        print("   - Language detection across multiple scripts")
        print("   - Tokenization with language-specific algorithms")
        print("   - Multi-language Named Entity Recognition")
        print("   - Script transliteration and translation")
        print("   - Field normalization and standardization")
        print("   - Cross-language entity matching")
        print("   - Batch and asynchronous processing")
        print("\nğŸš€ The system is ready for business intelligence processing!")

    except Exception as e:
        print(f"âŒ Demo failed with error: {e}")
        logger.exception("Demo execution failed")


async def async_demo():
    """Demonstrate async processing capabilities"""
    print_section("Async Processing Demo")

    try:
        from business_intel_scraper.backend.nlp.multilang import multilang_processor

        # Sample texts for async processing
        texts = [
            "Apple Inc. financial report",
            "Google revenue statistics",
            "Microsoft quarterly earnings",
        ]

        print("ğŸ”„ Processing texts asynchronously...")

        # Process texts asynchronously
        results = await multilang_processor.async_batch_process(texts)

        for i, result in enumerate(results):
            if hasattr(result, "detected_language"):
                print(
                    f"âš¡ Async result {i+1}: {result.detected_language.language.name}"
                )
            else:
                print(f"âŒ Async result {i+1}: Failed")

        print("âœ… Async processing completed!")

    except Exception as e:
        print(f"âŒ Async demo failed: {e}")


if __name__ == "__main__":
    # Run main demo
    main()

    # Run async demo
    print("\n" + "=" * 60)
    asyncio.run(async_demo())
